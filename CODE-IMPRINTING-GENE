###############################
rm(list=ls())
library(GSVA)
library(dorothea)
library(bcellViper)
library(dplyr)
library(viper)
library(GEOquery)
setwd("D:\\OneDrive")
load("OVDataSet.rda")

data(dorothea_hs,package="dorothea")
regulons = dorothea_hs %>%
  filter(confidence %in% c("A","B","C","D","E"))

#save(regulons,file = "regulons.rda")
i=1
OV_TF_activities<-list()
for(i in 1:length(OVDataSet)){
  dset=OVDataSet[[i]]
  tf_activities<-run_viper(dset,regulons,
                           options=list(method="scale",minsize = 4,
                                        eset.filter=FALSE, cores=1,verbose=FALSE))
  OV_TF_activities[[i]]<-tf_activities
  names(OV_TF_activities)[i]<-names(OVDataSet)[i]
}


save(OV_TF_activities,file="OV_TF_activities.rda")



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Model
from keras.layers import Dense, Dropout, Input
from pandas import read_csv
from sklearn.metrics import precision_recall_curve, auc
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection  import GridSearchCV
from sklearn.preprocessing import StandardScaler

seed=7
np.random.seed(seed)

OV_train = pd.read_csv('OV_train.tsv',header='infer',sep="\t")
train_label = pd.read_csv('label_train.tsv',header='infer',sep="\t")

#TCGA_dat = TCGA_TF_activities[TF]
train_dat =StandardScaler().fit_transform(OV_train)
#X = TCGA_dat
#Y = TCGA_label

#################################
X = train_dat
X = np.array(X)
Y = train_label
Y = np.array(Y)



input_size =1310

def create_model(optimizer='adam'):
    model = Sequential()
    model.add(Dense(1000,input_dim=input_size,activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(250,activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(50, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(1,activation='sigmoid'))
    
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model)
optimizers = ['adam','SGD']

epochs_1 = list([50,100,150])
batches = list([5,10,20])
param_grid = dict(optimizer=optimizers,epochs=epochs_1,batch_size=batches)

grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X, Y)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
canshu = grid_result.best_params_
opt = canshu.get('optimizer')
epo = canshu.get('epochs')
bat = canshu.get('batch_size')

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

model = KerasClassifier(build_fn=create_model, nb_epoch=epo, batch_size=bat)


kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(model, X, Y, cv=kfold)
print('Accuracy: %.2f (%.2f) MSE' % (results.mean(), results.std()))
print(results.mean())
result1 = pd.DataFrame(results)
result_mean = results.mean()
result1.iloc[9,0] = result_mean
result1.index = [1,2,3,4,5,6,7,8,9,'mean']
result1.columns = ['cross_val_score']
result1.to_csv('cross_val_score.tsv',sep="\t")



##########################
X = train_dat
X = np.array(X)
Y = train_label
Y = np.array(Y)

Y[Y == 1] = 0
Y[Y == 2] = 1

axes = plt.subplots(1,1,figsize=(5, 5))

predictor = KerasClassifier(build_fn=create_model, nb_epoch=epo, batch_size=bat)
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)

y_real = []
y_proba = []
for i, (train_index, test_index) in enumerate(kfold.split(X,Y)):
    Xtrain, Xtest = X[train_index], X[test_index]
    Ytrain, Ytest = Y[train_index], Y[test_index]
    predictor.fit(Xtrain, Ytrain)
    pred_proba = predictor.predict_proba(Xtest)
    #precision, recall, _ = precision_recall_curve(Ytest, pred_proba[:,1])
    #lab = 'Fold %d AUC=%.4f' % (i+1, auc(recall, precision))
    #axes[1].step(recall, precision, label=lab)
    y_real.append(Ytest)
    y_proba.append(pred_proba[:,1])

y_real = np.concatenate(y_real)
y_proba = np.concatenate(y_proba)
precision, recall, _ = precision_recall_curve(y_real, y_proba)
lab = 'Overall AUC=%.4f' % (auc(recall, precision))


axes[1].step((1-precision),recall,label=lab, lw=2, color='red')
axes[1].set_xlabel('1-Precision')
axes[1].set_ylabel('Recall')
axes[1].legend(loc='lower right', fontsize='small')

plt.show()

axes[0].savefig('result.pdf')


#######################################################
microarray analysis
# Version info: R 4.2.2, Biobase 2.58.0, GEOquery 2.66.0, limma 3.54.0
################################################################
#   Differential expression analysis with limma
library(GEOquery)
library(limma)
library(umap)

# load series and platform data from GEO

gset <- getGEO("GSE ", GSEMatrix =TRUE, AnnotGPL=FALSE)
if (length(gset) > 1) idx <- grep("GPL26963", attr(gset, "names")) else idx <- 1
gset <- gset[[idx]]

# make proper column names to match toptable 
fvarLabels(gset) <- make.names(fvarLabels(gset))

# group membership for all samples
gsms <- "0000110111"
sml <- strsplit(gsms, split="")[[1]]

# log2 transformation
ex <- exprs(gset)
qx <- as.numeric(quantile(ex, c(0., 0.25, 0.5, 0.75, 0.99, 1.0), na.rm=T))
LogC <- (qx[5] > 100) ||
          (qx[6]-qx[1] > 50 && qx[2] > 0)
if (LogC) { ex[which(ex <= 0)] <- NaN
  exprs(gset) <- log2(ex) }

# assign samples to groups and set up design matrix
gs <- factor(sml)
groups <- make.names(c("contrl","rif"))
levels(gs) <- groups
gset$group <- gs
design <- model.matrix(~group + 0, gset)
colnames(design) <- levels(gs)

gset <- gset[complete.cases(exprs(gset)), ] # skip missing values

fit <- lmFit(gset, design)  # fit linear model

# set up contrasts of interest and recalculate model coefficients
cts <- paste(groups[1], groups[2], sep="-")
cont.matrix <- makeContrasts(contrasts=cts, levels=design)
fit2 <- contrasts.fit(fit, cont.matrix)

# compute statistics and table of top significant genes
fit2 <- eBayes(fit2, 0.01)
tT <- topTable(fit2, adjust="fdr", sort.by="B", number=250)

tT <- subset(tT, select=c("ID","adj.P.Val","P.Value","t","B","logFC","ORF","SPOT_ID"))
write.table(tT, file=stdout(), row.names=F, sep="\t")

# Visualize and quality control test results.
# Build histogram of P-values for all genes. Normal test
# assumption is that most genes are not differentially expressed.
tT2 <- topTable(fit2, adjust="fdr", sort.by="B", number=Inf)
hist(tT2$adj.P.Val, col = "grey", border = "white", xlab = "P-adj",
  ylab = "Number of genes", main = "P-adj value distribution")

# summarize test results as "up", "down" or "not expressed"
dT <- decideTests(fit2, adjust.method="fdr", p.value=0.05, lfc=1.5)

# Venn diagram of results
vennDiagram(dT, circle.col=palette())

# create Q-Q plot for t-statistic
t.good <- which(!is.na(fit2$F)) # filter out bad probes
qqt(fit2$t[t.good], fit2$df.total[t.good], main="Moderated t statistic")

# volcano plot (log P-value vs log fold change)
colnames(fit2) # list contrast names
ct <- 1        # choose contrast of interest
# Please note that the code provided to generate graphs serves as a guidance to
# the users. It does not replicate the exact GEO2R web display due to multitude
# of graphical options.
# 
# The following will produce basic volcano plot using limma function:
volcanoplot(fit2, coef=ct, main=colnames(fit2)[ct], pch=20,
  highlight=length(which(dT[,ct]!=0)), names=rep('+', nrow(fit2)))

# MD plot (log fold change vs mean log expression)
# highlight statistically significant (p-adj < 0.05) probes
plotMD(fit2, column=ct, status=dT[,ct], legend=F, pch=20, cex=1)
abline(h=0)

################################################################
# General expression data analysis
ex <- exprs(gset)

# box-and-whisker plot
ord <- order(gs)  # order samples by group
palette(c("#1B9E77", "#7570B3", "#E7298A", "#E6AB02", "#D95F02",
          "#66A61E", "#A6761D", "#B32424", "#B324B3", "#666666"))
par(mar=c(7,4,2,1))
title <- paste ("GSE188409", "/", annotation(gset), sep ="")
boxplot(ex[,ord], boxwex=0.6, notch=T, main=title, outline=FALSE, las=2, col=gs[ord])
legend("topleft", groups, fill=palette(), bty="n")

# expression value distribution
par(mar=c(4,4,2,1))
title <- paste ("GSE188409", "/", annotation(gset), " value distribution", sep ="")
plotDensities(ex, group=gs, main=title, legend ="topright")

# UMAP plot (dimensionality reduction)
ex <- na.omit(ex) # eliminate rows with NAs
ex <- ex[!duplicated(ex), ]  # remove duplicates
ump <- umap(t(ex), n_neighbors = 5, random_state = 123)
par(mar=c(3,3,2,6), xpd=TRUE)
plot(ump$layout, main="UMAP plot, nbrs=5", xlab="", ylab="", col=gs, pch=20, cex=1.5)
legend("topright", inset=c(-0.15,0), legend=levels(gs), pch=20,
col=1:nlevels(gs), title="Group", pt.cex=1.5)
library("maptools")  # point labels without overlaps
pointLabel(ump$layout, labels = rownames(ump$layout), method="SANN", cex=0.6)

# mean-variance trend, helps to see if precision weights are needed
plotSA(fit2, main="Mean variance trend, GSE188409")





